{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from bald_latin import remove_cyrillic_and_accents\n",
    "from dataset import UnlabeledDataset, FoldDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the original and stemmed comments, and their labels.\n",
    "Remove the cyrillic comments and remove accents from ć,č,ž,š,đ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "originals = open('dataset/lns/lns_comments.txt', 'r').readlines()\n",
    "stemmed = open('dataset/lns/lns_comments_stemmed.txt', 'r').readlines()\n",
    "labels = open('dataset/lns/lns_labels.txt', 'r').readlines()\n",
    "\n",
    "# scraped\n",
    "scraped_comments = open('dataset/scraped/comments.txt').readlines()\n",
    "scraped_stemmed  = open('dataset/scraped/comments_stemmed.txt').readlines()\n",
    "scraped = UnlabeledDataset(scraped_comments, scraped_stemmed)\n",
    "nots = open('dataset/scraped/slobodno_vreme.txt')\n",
    "\n",
    "# remove cyrillic and balden text\n",
    "stemmed, labels = remove_cyrillic_and_accents(stemmed, labels)\n",
    "originals, _ = remove_cyrillic_and_accents(originals, range(len(originals)), remove_accents=False)\n",
    "\n",
    "# labels as a numpy array\n",
    "labels = np.array([int(float(x)) for x in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Naive Bayes classifier using K-Fold crossvalidation\n",
    "### Test it on each test set, average the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "croatian_stop_words = set([u\"a\",u\"ako\",u\"ali\",u\"bi\",u\"bih\",u\"bila\",u\"bili\",u\"bilo\",u\"bio\",u\"bismo\",u\"biste\",u\"biti\",u\"bumo\",u\"da\",u\"do\",u\"duž\",u\"ga\",u\"hoće\",u\"hoćemo\",u\"hoćete\",u\"hoćeš\",u\"hoću\",u\"i\",u\"iako\",u\"ih\",u\"ili\",u\"iz\",u\"ja\",u\"je\",u\"jedna\",u\"jedne\",u\"jedno\",u\"jer\",u\"jesam\",u\"jesi\",u\"jesmo\",u\"jest\",u\"jeste\",u\"jesu\",u\"jim\",u\"joj\",u\"još\",u\"ju\",u\"kada\",u\"kako\",u\"kao\",u\"koja\",u\"koje\",u\"koji\",u\"kojima\",u\"koju\",u\"kroz\",u\"li\",u\"me\",u\"mene\",u\"meni\",u\"mi\",u\"mimo\",u\"moj\",u\"moja\",u\"moje\",u\"mu\",u\"na\",u\"nad\",u\"nakon\",u\"nam\",u\"nama\",u\"nas\",u\"naš\",u\"naša\",u\"naše\",u\"našeg\",u\"ne\",u\"nego\",u\"neka\",u\"neki\",u\"nekog\",u\"neku\",u\"nema\",u\"netko\",u\"neće\",u\"nećemo\",u\"nećete\",u\"nećeš\",u\"neću\",u\"nešto\",u\"ni\",u\"nije\",u\"nikoga\",u\"nikoje\",u\"nikoju\",u\"nisam\",u\"nisi\",u\"nismo\",u\"niste\",u\"nisu\",u\"njega\",u\"njegov\",u\"njegova\",u\"njegovo\",u\"njemu\",u\"njezin\",u\"njezina\",u\"njezino\",u\"njih\",u\"njihov\",u\"njihova\",u\"njihovo\",u\"njim\",u\"njima\",u\"njoj\",u\"nju\",u\"no\",u\"o\",u\"od\",u\"odmah\",u\"on\",u\"ona\",u\"oni\",u\"ono\",u\"ova\",u\"pa\",u\"pak\",u\"po\",u\"pod\",u\"pored\",u\"prije\",u\"s\",u\"sa\",u\"sam\",u\"samo\",u\"se\",u\"sebe\",u\"sebi\",u\"si\",u\"smo\",u\"ste\",u\"su\",u\"sve\",u\"svi\",u\"svog\",u\"svoj\",u\"svoja\",u\"svoje\",u\"svom\",u\"ta\",u\"tada\",u\"taj\",u\"tako\",u\"te\",u\"tebe\",u\"tebi\",u\"ti\",u\"to\",u\"toj\",u\"tome\",u\"tu\",u\"tvoj\",u\"tvoja\",u\"tvoje\",u\"u\",u\"uz\",u\"vam\",u\"vama\",u\"vas\",u\"vaš\",u\"vaša\",u\"vaše\",u\"već\",u\"vi\",u\"vrlo\",u\"za\",u\"zar\",u\"će\",u\"ćemo\",u\"ćete\",u\"ćeš\",u\"ću\",u\"što\"])\n",
    "\n",
    "# build tf-idf vectorizer which uses unigrams and bigrams.\n",
    "# uses words with 2+ occurances as features\n",
    "vectorizer = TfidfVectorizer(\n",
    "    strip_accents=\"unicode\",\n",
    "    lowercase=True,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=10,\n",
    "    norm='l2',\n",
    "    smooth_idf=True,\n",
    "    use_idf=True,\n",
    "    stop_words=croatian_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be type, not FoldDataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-91b8be58bbcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# build the dataset, vectorize it using TF-IDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFoldDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comments_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comments_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/mihailo/development/projects/botshot/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, comments, stemmed, labels, train_indices, test_indices)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mcomments_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstemmed_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmed_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comments_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be type, not FoldDataset"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "nums = []\n",
    "\n",
    "\n",
    "# Split the dataset into k folds, each fold is a test set for one iteration\n",
    "k_fold = KFold(len(stemmed), n_folds=n_folds, shuffle=False)\n",
    "for fold_count, (train, test) in enumerate(k_fold):\n",
    "    \n",
    "    # build the dataset, vectorize it using TF-IDF\n",
    "    data = FoldDataset(originals, stemmed, labels, train, test)\n",
    "    data.X_train = vectorizer.fit_transform(data._comments_train)\n",
    "    data.X_test  = vectorizer.transform(data._comments_test)\n",
    "        \n",
    "    # create and fit the classifier\n",
    "    clf = MultinomialNB().fit(data.X_train, data.y_train)\n",
    "    \n",
    "    proba = clf.predict_proba(data.X_test)[:,1]\n",
    "    \n",
    "    # print precision for different thresholds\n",
    "    thresholds = np.linspace(0.5, 1)\n",
    "    accuracy, precision, recall, num_classified = [], [], [], []\n",
    "    for threshold in thresholds:\n",
    "        results = data.test_prediction_above(proba, threshold, print_options=[])\n",
    "        accuracy.append(results[0]); precision.append(results[1]); recall.append(results[2]); \n",
    "        num_classified.append(np.sum(np.logical_or(proba < 1 - threshold, proba > threshold)))\n",
    "            \n",
    "    fig = plt.figure(1, figsize=(13, 8))\n",
    "#     plt.plot(thresholds, accuracy, label=\"Accuracy\" if fold_count == 0 else None, color='b', alpha=0.5)\n",
    "    plt.plot(thresholds, precision, label=\"Precision\" if not fold_count else None, color='r')\n",
    "    plt.plot(thresholds, recall, label=\"Recall\" if not fold_count else None, color='g')\n",
    "    plt.legend(loc='upper left')\n",
    "    nums.append(num_classified)\n",
    "\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.twinx()\n",
    "for num in nums:\n",
    "    plt.plot(thresholds, num, label=\"Number of comments classified\", color='k', alpha=0.5)\n",
    "\n",
    "plt.title(\"Precision and recall vs the number of comments with a classification probability above threshold\")\n",
    "plt.ylabel(\"Number of comments classified\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have picked a threshold for bot classification, lets classify the scraped comments\n",
    "### With the scraped comments, add the scraped not-bots and train an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "threshold = 0.84\n",
    "max_scraped = 10**5\n",
    "\n",
    "# Split the dataset into k folds, each fold is a test set for one iteration\n",
    "k_fold = KFold(len(corpus), n_folds=n_folds, shuffle=False)\n",
    "for fold_count, (train, test) in enumerate(k_fold):\n",
    "    corpus_train = [corpus[x] for x in train]\n",
    "    corpus_test = [corpus[x] for x in test]\n",
    "    labels_train = [labels[x] for x in train]\n",
    "    labels_test = [labels[x] for x in test]\n",
    "    \n",
    "    # build the dataset, vectorize it using TF-IDF\n",
    "    data = CommentDataset(corpus_train, labels_train, corpus_test, labels_test)\n",
    "    data.X_train = vectorizer.fit_transform(data._comments_train)\n",
    "    data.X_test  = vectorizer.transform(data._comments_test)\n",
    "    scraped.X = vectorizer.transform(scraped.comments[:max_scraped])\n",
    "        \n",
    "    # create and fit the classifier\n",
    "    clf = MultinomialNB().fit(data.X_train, data.y_train)\n",
    "    \n",
    "    proba = clf.predict_proba(scraped.X)[:,1]\n",
    "    indices = list(np.argwhere(proba > threshold).flatten())\n",
    "    print \"Classified {} comments as bots\".format(len(indices))\n",
    "    \n",
    "    scraped.bots = [scraped.comments[x] for x in indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label a dataset from the scraped comments and train the network on it\n",
    "### For each fold, train a separate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_folds=3\n",
    "values = np.zeros(3)  # accuracy, precision, recall\n",
    "probabilities = []\n",
    "\n",
    "# Split the dataset into k folds, each fold is a test set for one iteration\n",
    "k_fold = KFold(len(corpus), n_folds=n_folds, shuffle=True)\n",
    "for fold_count, (train, test) in enumerate(k_fold):\n",
    "#     print \"Fold #{}\".format(fold_count)\n",
    "    corpus_train = [corpus[x] for x in train]\n",
    "    corpus_test = [corpus[x] for x in test]\n",
    "    labels_train = [labels[x] for x in train]\n",
    "    labels_test = [labels[x] for x in test]\n",
    "    \n",
    "    # build the dataset, vectorize it using TF-IDF\n",
    "    data = CommentDataset(corpus_train, labels_train, corpus_test, labels_test)\n",
    "    data.vectorize_fit()\n",
    "        \n",
    "    # create and fit the classifier\n",
    "    clf = MultinomialNB().fit(data.X_train, data.y_train)\n",
    "    \n",
    "    proba = clf.predict_proba(data.X_test)[:,1]\n",
    "    probabilities.append(proba)\n",
    "    \n",
    "    values += data.test_prediction(proba, print_options=[])[:3]  # for averaging metrics on k-folds\n",
    "    \n",
    "print \"Results on {} folds:\".format(fold_count + 1)\n",
    "print (\"Accuracy: {},\\nPrecision: {},\\nRecall: {}\".format(values[0]/n_folds, values[1]/n_folds, values[2]/n_folds))\n",
    "\n",
    "\n",
    "# plots\n",
    "plt.figure(figsize=(13, 4))\n",
    "for fold, proba in enumerate(probabilities):\n",
    "    n, bins, patches = plt.hist(proba, 100, alpha=0.5, label=\"Fold #{}\".format(fold))\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
